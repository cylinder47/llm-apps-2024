{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3.2 Corrective RAG\n",
   "id": "81c2ce3742ccdccb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Setup\n",
    "\n",
    "### Install dependencies"
   ],
   "id": "8863b270dbdeceb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%pip install python-dotenv~=1.0 docarray~=0.40.0 pypdf~=5.1 --upgrade --quiet\n",
    "%pip install chromadb~=0.5.18 sentence-transformers~=3.3 --upgrade --quiet \n",
    "%pip install langchain~=0.3.7 langchain_openai~=0.2.6 langchain_community~=0.3.5 langchain-chroma~=0.1.4 langchainhub~=0.1.21 --upgrade --quiet\n",
    "\n",
    "# If running locally, you can do this instead:\n",
    "#%pip install -r ../requirements.txt"
   ],
   "id": "cdb7524edadc7d97",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load environment variables",
   "id": "9167196e4bd4669f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# If running in Google Colab, you can use this code instead:\n",
    "# from google.colab import userdata\n",
    "# os.environ[\"AZURE_OPENAI_API_KEY\"] = userdata.get(\"AZURE_OPENAI_API_KEY\")\n",
    "# os.environ[\"AZURE_OPENAI_ENDPOINT\"] = userdata.get(\"AZURE_OPENAI_ENDPOINT\")"
   ],
   "id": "be489920b631789d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setup models",
   "id": "8dbfb71250edea80"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\n",
    "api_version = \"2024-10-01-preview\"\n",
    "llm = AzureChatOpenAI(deployment_name=\"gpt-4o\", temperature=0.0, api_version=api_version)\n",
    "embedding_model = AzureOpenAIEmbeddings(model=\"text-embedding-3-large\", api_version=api_version)"
   ],
   "id": "36a32dc38f9ac0d3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setup LangSmith tracing for this notebook",
   "id": "93340862e2903b4d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "# API key etc is in the .env file\n",
    "# my_name = \"Totoro\"\n",
    "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "# os.environ[\"LANGCHAIN_PROJECT\"] = f\"tokyo24-test-{my_name}\""
   ],
   "id": "1543c27220eddad5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setup path to data ",
   "id": "2fe1b093890b90c2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "data_path = \"../data\"",
   "id": "4afff1ee6e9613d6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# CRAG\n",
    "\n",
    "Corrective-RAG (CRAG) is a recent paper that introduces an interesting approach for active RAG, incorporating self-reflection / self-grading on retrieved documents. \n",
    "\n",
    "The general ideas in the [paper](https://arxiv.org/pdf/2401.15884.pdf) are:\n",
    "\n",
    "* Retrieve documents\n",
    "* Evaluate them for relevance to the user question\n",
    "* Perform knowledge refinement\n",
    "* Use web search as a fallback if any documents are not relevant\n",
    "* The diagrams in the paper also suggest that query re-writing is used here"
   ],
   "id": "4abb5d6c1a391b72"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "![CRAG.png](https://www.dropbox.com/scl/fi/fnpz0n5unnr3iadw1ob2n/crag.png?rlkey=yz9ft5aq87aibnace7f3g3xav&dl=1)\n",
    "\n",
    "**Paper:**\n",
    "https://arxiv.org/pdf/2401.15884.pdf"
   ],
   "id": "bf740cefc5cd7561"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Let's implement this from scratch using LangChain.\n",
    "\n",
    "We can make some simplifications:\n",
    "\n",
    "* Let's skip the knowledge refinement phase as a first pass. This can be added back as a node, if desired. \n",
    "* If *any* document is irrelevant, let's opt to supplement retrieval with web search. \n",
    "* We'll simulate web search using an LLM-query (this can actually be a viable alternative fallback). \n",
    "* Let's use query re-writing to optimize the query for web search.\n"
   ],
   "id": "cae10285b9bdc8f7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![CRAG flow](https://www.dropbox.com/scl/fi/m6x5vsxvb5p67i89stccq/crag-flow.png?rlkey=bkjz3gomb4cn2sx9iyrr3s0e6&dl=1)",
   "id": "9d262527d874c603"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Retriever\n",
    " \n",
    "Let's index 3 blog posts."
   ],
   "id": "3efb1d7d6891d61"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=250, chunk_overlap=0\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "# Add to vectorDB\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=doc_splits,\n",
    "    collection_name=\"rag-chroma\",\n",
    "    embedding=embedding_model,\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ],
   "id": "8b6c6527730f6178",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Retrieval Grader",
   "id": "35abe23a64368c17"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Data model\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(description=\"Documents are relevant to the question, 'yes' or 'no'\")"
   ],
   "id": "1282f49811dbdef0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Prompt \n",
    "system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "    If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
    "    ]\n",
    ")"
   ],
   "id": "66cdc90e6d11809a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
    "retrieval_grader = grade_prompt | structured_llm_grader\n",
    "question = \"agent memory\"\n",
    "docs = retriever.invoke(question)\n",
    "doc_txt = docs[1].page_content\n",
    "print(retrieval_grader.invoke({\"question\": question, \"document\": doc_txt}))"
   ],
   "id": "b711fcbaaddf32e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Define a function for grading documents",
   "id": "ef1ea7f5d3029bcf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from typing import Dict, Any\n",
    "\n",
    "def grade_documents(input) -> Dict[str, Any]: \n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = input[\"question\"]\n",
    "    documents = input[\"documents\"]\n",
    "\n",
    "    # Score each doc\n",
    "    filtered_docs = []\n",
    "    web_search = \"no\"\n",
    "    for d in documents:\n",
    "        # TODO: Consider batching\n",
    "        score = retrieval_grader.invoke({\"question\": question, \"document\": d.page_content})\n",
    "        grade = score.binary_score\n",
    "        # Document relevant\n",
    "        if grade.lower() == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        # Document not relevant\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT - WEB SEARCH NEEDED---\")\n",
    "            # We do not include the document in filtered_docs\n",
    "            # We set a flag to indicate that we want to run web search\n",
    "            web_search = \"yes\"\n",
    "            continue\n",
    "    return {\"documents\": filtered_docs, \"question\": question, \"web_search\": web_search}"
   ],
   "id": "1b5174cf4979c7a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Generate",
   "id": "d128628f1587a6e7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Prompt\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Chain\n",
    "rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Run\n",
    "generation = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "print(generation)"
   ],
   "id": "df1f30fd0c0b3a3c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Build the final chains",
   "id": "d0061e1a2d118572"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda, RunnableBranch\n",
    "\n",
    "retriever_and_grading_chain = ({ # This is a shorthand for a RunnableMap / RunnableParallel\n",
    "        \"documents\": retriever,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | RunnableLambda(lambda x: grade_documents(x))\n",
    ")\n",
    "\n",
    "web_search_chain_partial = (ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", \"You are an Icelandic Viking from the 11th century, with no knowledge about modern technology. Confidently pretend that you know the answer to the user's question. Always end with a short tale about an heroic deed, seemingly related to the question.\"),\n",
    "            (\"human\", \"{question}\"),\n",
    "        ]\n",
    "    ) \n",
    "    | llm.bind(temperature=0.9) \n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "web_search_chain = RunnablePassthrough.assign(context=web_search_chain_partial) \n",
    "\n",
    "branch_chain = (RunnableBranch(\n",
    "    (lambda x: x[\"web_search\"] == \"yes\", web_search_chain | rag_chain),\n",
    "    RunnableLambda(lambda x: {\"context\": format_docs(x[\"documents\"]), \"question\": x[\"question\"]}) | rag_chain\n",
    "    )\n",
    ")\n",
    "\n",
    "final_chain = retriever_and_grading_chain | branch_chain\n",
    "\n",
    "final_chain.invoke(\"agent memory\")"
   ],
   "id": "345af10d4596e19a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
