{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3.4 Parent document retriever\n",
   "id": "6742288ca2995845"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Setup\n",
    "\n",
    "### Install dependencies"
   ],
   "id": "36c18d66b5c8cd59"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%pip install python-dotenv~=1.0 docarray~=0.40.0 pypdf~=5.1 --upgrade --quiet\n",
    "%pip install chromadb~=0.5.18 sentence-transformers~=3.3 --upgrade --quiet \n",
    "%pip install langchain~=0.3.7 langchain_openai~=0.2.6 langchain_community~=0.3.5 langchain-chroma~=0.1.4 langchainhub~=0.1.21 --upgrade --quiet\n",
    "%pip install langchain_experimental~=0.3.3 --upgrade --quiet\n",
    "\n",
    "# If running locally, you can do this instead:\n",
    "#%pip install -r ../requirements.txt"
   ],
   "id": "ff3373aec472b2ba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load environment variables",
   "id": "6f15dfd6c30f8ba1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# If running in Google Colab, you can use this code instead:\n",
    "# from google.colab import userdata\n",
    "# os.environ[\"AZURE_OPENAI_API_KEY\"] = userdata.get(\"AZURE_OPENAI_API_KEY\")\n",
    "# os.environ[\"AZURE_OPENAI_ENDPOINT\"] = userdata.get(\"AZURE_OPENAI_ENDPOINT\")"
   ],
   "id": "f37880b7bb1ee2df",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setup models",
   "id": "afb182ba58283160"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\n",
    "api_version = \"2024-10-01-preview\"\n",
    "llm = AzureChatOpenAI(deployment_name=\"gpt-4o\", temperature=0.0, openai_api_version=api_version)\n",
    "embedding_model = AzureOpenAIEmbeddings(model=\"text-embedding-3-large\", openai_api_version=api_version)"
   ],
   "id": "ebd34bf3bdd2461d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setup LangSmith tracing for this notebook",
   "id": "7a216300ae05a985"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "# API key etc is in the .env file\n",
    "# my_name = \"Totoro\"\n",
    "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "# os.environ[\"LANGCHAIN_PROJECT\"] = f\"tokyo24-test-{my_name}\""
   ],
   "id": "a938ce3b2a2308d5",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "34883374",
   "metadata": {},
   "source": [
    "# How to use the Parent Document Retriever\n",
    "\n",
    "When splitting documents for retrieval, there are often conflicting desires:\n",
    "\n",
    "1. You may want to have small documents, so that their embeddings can most\n",
    "    accurately reflect their meaning. If too long, then the embeddings can\n",
    "    lose meaning.\n",
    "2. You want to have long enough documents that the context of each chunk is\n",
    "    retained.\n",
    "\n",
    "The `ParentDocumentRetriever` strikes that balance by splitting and storing\n",
    "small chunks of data. During retrieval, it first fetches the small chunks\n",
    "but then looks up the parent ids for those chunks and returns those larger\n",
    "documents.\n",
    "\n",
    "**Note** that \"parent document\" refers to the document that a small chunk\n",
    "originated from. This can either be the whole raw document **OR a larger\n",
    "chunk**."
   ]
  },
  {
   "cell_type": "code",
   "id": "8b6e74b2",
   "metadata": {},
   "source": [
    "from langchain.retrievers import ParentDocumentRetriever"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain.storage import InMemoryStore\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ],
   "id": "1d17af96",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "604ff981",
   "metadata": {},
   "source": [
    "loaders = [\n",
    "    TextLoader(\"../data/paul_graham_essay.txt\"),\n",
    "    TextLoader(\"../data/state_of_the_union.txt\"),\n",
    "]\n",
    "docs = []\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d3943f72",
   "metadata": {},
   "source": [
    "## Retrieving full documents\n",
    "\n",
    "In this mode, we want to retrieve the full documents. Therefore, we only specify a child splitter."
   ]
  },
  {
   "cell_type": "code",
   "id": "1a8b2e5f",
   "metadata": {},
   "source": [
    "# This text splitter is used to create the child documents\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n",
    "# The vectorstore to use to index the child chunks\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"full_documents\", embedding_function=embedding_model\n",
    ")\n",
    "# The storage layer for the parent documents (using an in-memory store for simplicity)\n",
    "store = InMemoryStore()\n",
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2b107935",
   "metadata": {},
   "source": [
    "retriever.add_documents(docs, ids=None)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d05b97b7",
   "metadata": {},
   "source": [
    "This should yield two keys, because we added two documents."
   ]
  },
  {
   "cell_type": "code",
   "id": "30e3812b",
   "metadata": {},
   "source": [
    "list(store.yield_keys())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f895d62b",
   "metadata": {},
   "source": [
    "Let's now call the vector store search functionality - we should see that it returns small chunks (since we're storing the small chunks)."
   ]
  },
  {
   "cell_type": "code",
   "id": "b261c02c",
   "metadata": {},
   "source": [
    "sub_docs = vectorstore.similarity_search(\"justice breyer\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5108222f",
   "metadata": {},
   "source": [
    "print(sub_docs[0].page_content)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bda8ed5a",
   "metadata": {},
   "source": "Let's now retrieve from the overall **retriever**. This should return large documents - since it returns the documents where the smaller chunks are located."
  },
  {
   "cell_type": "code",
   "id": "419a91c4",
   "metadata": {},
   "source": [
    "retrieved_docs = retriever.invoke(\"justice breyer\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cf10d250",
   "metadata": {},
   "source": [
    "len(retrieved_docs[0].page_content)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "14f813a5",
   "metadata": {},
   "source": [
    "## Retrieving larger chunks\n",
    "\n",
    "Sometimes, the full documents can be too big to want to retrieve them as is. In that case, what we really want to do is to first split the raw documents into larger chunks, and then split it into smaller chunks. We then index the smaller chunks, but on retrieval we retrieve the larger chunks (but still not the full documents)."
   ]
  },
  {
   "cell_type": "code",
   "id": "b6f9a4f0",
   "metadata": {},
   "source": [
    "# This text splitter is used to create the parent documents\n",
    "parent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000)\n",
    "# This text splitter is used to create the child documents\n",
    "# It should create documents smaller than the parent\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n",
    "# The vectorstore to use to index the child chunks\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"split_parents\", embedding_function=embedding_model\n",
    ")\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryStore()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "19478ff3",
   "metadata": {},
   "source": [
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    "    parent_splitter=parent_splitter,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fe16e620",
   "metadata": {},
   "source": [
    "retriever.add_documents(docs)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "64ad3c8c",
   "metadata": {},
   "source": [
    "We can see that there are much more than two documents now - these are the larger chunks."
   ]
  },
  {
   "cell_type": "code",
   "id": "24d81886",
   "metadata": {},
   "source": [
    "len(list(store.yield_keys()))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "baaef673",
   "metadata": {},
   "source": [
    "Let's make sure the underlying vector store still retrieves the small chunks."
   ]
  },
  {
   "cell_type": "code",
   "id": "b1c859de",
   "metadata": {},
   "source": [
    "sub_docs = vectorstore.similarity_search(\"justice breyer\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6fffa2eb",
   "metadata": {},
   "source": [
    "print(sub_docs[0].page_content)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3a3202df",
   "metadata": {},
   "source": [
    "retrieved_docs = retriever.invoke(\"justice breyer\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "684fdb2c",
   "metadata": {},
   "source": [
    "len(retrieved_docs[0].page_content)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9f17f662",
   "metadata": {},
   "source": [
    "print(retrieved_docs[0].page_content)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
