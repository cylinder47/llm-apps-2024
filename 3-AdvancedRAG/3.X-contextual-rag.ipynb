{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3.X Contextual Retrieval\n",
   "id": "1038d76d0d5d46ab"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Setup\n",
    "\n",
    "### Install dependencies"
   ],
   "id": "6032e5c01675c224"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%pip install python-dotenv~=1.0 docarray~=0.40.0 --upgrade --quiet\n",
    "%pip install chromadb~=0.5.18 sentence-transformers~=2.7 --upgrade --quiet \n",
    "%pip install langchain~=0.3.7 langchain_openai~=0.2.6 langchain_community~=0.3.5 langchain-chroma~=0.1.4 langchainhub~=0.1.21 --upgrade --quiet\n",
    "%pip install langchain_experimental~=0.3.3 rank_bm25~=0.2.2 --upgrade --quiet\n",
    "%pip install flashRank~=0.2.9 ragatouille~=0.0.8 --upgrade --quiet\n",
    "\n",
    "# If running locally, you can do this instead:\n",
    "#%pip install -r ../requirements.txt"
   ],
   "id": "a17a5d4eb87aeb30",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load environment variables",
   "id": "f6a0787445b6a310"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# If running in Google Colab, you can use this code instead:\n",
    "# from google.colab import userdata\n",
    "# os.environ[\"AZURE_OPENAI_API_KEY\"] = userdata.get(\"AZURE_OPENAI_API_KEY\")\n",
    "# os.environ[\"AZURE_OPENAI_ENDPOINT\"] = userdata.get(\"AZURE_OPENAI_ENDPOINT\")"
   ],
   "id": "34802da25cdc7141",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setup models",
   "id": "4b944c93ac730c33"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\n",
    "api_version = \"2024-10-01-preview\"\n",
    "llm = AzureChatOpenAI(deployment_name=\"gpt-4o\", temperature=0.0, api_version=api_version)\n",
    "embedding_model = AzureOpenAIEmbeddings(model=\"text-embedding-3-large\", api_version=api_version)"
   ],
   "id": "8802fce3cdc2c804",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setup LangSmith tracing for this notebook",
   "id": "402e1ccea85fc3ba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "# API key etc is in the .env file\n",
    "# my_name = \"Totoro\"\n",
    "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "# os.environ[\"LANGCHAIN_PROJECT\"] = f\"tokyo24-test-{my_name}\""
   ],
   "id": "51def92e18fe51c6",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c6bf25fda89f8656",
   "metadata": {},
   "source": [
    "# Intro\n",
    "\n",
    "In this notebook we will showcase how you can implement Anthropic's [Contextual Retrieval](https://www.anthropic.com/news/contextual-retrieval) using LangChain. Contextual Retrieval addresses the conundrum of traditional RAG approaches by prepending chunk-specific explanatory context to each chunk before embedding.\n",
    "\n",
    "![](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F2496e7c6fedd7ffaa043895c23a4089638b0c21b-3840x2160.png&w=3840&q=75)"
   ]
  },
  {
   "cell_type": "code",
   "id": "a4490b37e0479034",
   "metadata": {},
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "logging.disable(level=logging.INFO)\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f75da2885a562f6d",
   "metadata": {},
   "source": [
    "## Download Data\n",
    "\n",
    "We will use `Paul Graham Essay` dataset."
   ]
  },
  {
   "cell_type": "code",
   "id": "99266f4b27564077",
   "metadata": {},
   "source": "#wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt' -O '../data/paul_graham_essay.txt'",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "23200549ef2260bb",
   "metadata": {},
   "source": "## Setup LLM and Embedding model"
  },
  {
   "cell_type": "markdown",
   "id": "e29fcd718472faca",
   "metadata": {},
   "source": "## Load Data"
  },
  {
   "cell_type": "code",
   "id": "a429c5e9806687c2",
   "metadata": {},
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "loader = TextLoader(\"../data/paul_graham_essay.txt\")\n",
    "documents = loader.load()\n",
    "WHOLE_DOCUMENT = documents[0].page_content"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1e8beefd063110bc",
   "metadata": {},
   "source": [
    "## Prompts for creating context for each chunk\n",
    "\n",
    "We will use the following prompts to create chunk-specific explanatory context to each chunk before embedding."
   ]
  },
  {
   "cell_type": "code",
   "id": "51131f316c3c4dc1",
   "metadata": {},
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "prompt_document = PromptTemplate(\n",
    "    input_variables=[\"WHOLE_DOCUMENT\"], template=\"{WHOLE_DOCUMENT}\"\n",
    ")\n",
    "prompt_chunk = PromptTemplate(\n",
    "    input_variables=[\"CHUNK_CONTENT\"],\n",
    "    template=\"Here is the chunk we want to situate within the whole document\\n\\n{CHUNK_CONTENT}\\n\\n\"\n",
    "    \"Please give a short succinct context to situate this chunk within the overall document for \"\n",
    "    \"the purposes of improving search retrieval of the chunk. Answer only with the succinct context and nothing else.\",\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ae226af17efd9663",
   "metadata": {},
   "source": "## Retrievers"
  },
  {
   "cell_type": "code",
   "id": "1565407255685439",
   "metadata": {},
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import BaseDocumentCompressor\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.retrievers import BM25Retriever, ContextualCompressionRetriever\n",
    "\n",
    "def split_text(texts):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_overlap=200)\n",
    "    doc_chunks = text_splitter.create_documents(texts)\n",
    "    for i, doc in enumerate(doc_chunks):\n",
    "        # Append a new Document object with the appropriate doc_id\n",
    "        doc.metadata = {\"doc_id\": f\"doc_{i}\"}\n",
    "    return doc_chunks\n",
    "\n",
    "\n",
    "def create_embedding_retriever(documents_):\n",
    "    vector_store = Chroma.from_documents(documents_, embedding=embedding_model)\n",
    "    return vector_store.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "\n",
    "def create_bm25_retriever(documents_):\n",
    "    retriever = BM25Retriever.from_documents(documents_, language=\"english\")\n",
    "    return retriever\n",
    "\n",
    "\n",
    "# Function to create a combined embedding and BM25 retriever with reranker\n",
    "class EmbeddingBM25RerankerRetriever:\n",
    "    def __init__(\n",
    "        self,\n",
    "        vector_retriever: BaseRetriever,\n",
    "        bm25_retriever: BaseRetriever,\n",
    "        reranker: BaseDocumentCompressor,\n",
    "    ):\n",
    "        self.vector_retriever = vector_retriever\n",
    "        self.bm25_retriever = bm25_retriever\n",
    "        self.reranker = reranker\n",
    "\n",
    "    def invoke(self, query: str):\n",
    "        vector_docs = self.vector_retriever.invoke(query)\n",
    "        bm25_docs = self.bm25_retriever.invoke(query)\n",
    "\n",
    "        combined_docs = vector_docs + [\n",
    "            doc for doc in bm25_docs if doc not in vector_docs\n",
    "        ]\n",
    "\n",
    "        reranked_docs = self.reranker.compress_documents(combined_docs, query)\n",
    "        return reranked_docs"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "37708e8a15bbef35",
   "metadata": {},
   "source": "### Non-contextual retrievers & reranking"
  },
  {
   "cell_type": "code",
   "id": "a85c21f8b344438c",
   "metadata": {},
   "source": [
    "chunks = split_text([WHOLE_DOCUMENT])\n",
    "\n",
    "embedding_retriever = create_embedding_retriever(chunks)\n",
    "\n",
    "# Define a BM25 retriever\n",
    "bm25_retriever = create_bm25_retriever(chunks)\n",
    "\n",
    "# For this example, we're using a a quick and simple reranker (see https://python.langchain.com/docs/integrations/retrievers/flashrank-reranker/)\n",
    "from langchain.retrievers.document_compressors import FlashrankRerank\n",
    "reranker = FlashrankRerank()\n",
    "\n",
    "# Alternatives:\n",
    "\n",
    "# 1. RAGatouille (see https://python.langchain.com/docs/integrations/providers/ragatouille/)\n",
    "# from ragatouille import RAGPretrainedModel\n",
    "# ragatouille_reranker = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
    "# reranker = ragatouille_reranker.as_langchain_document_compressor()\n",
    "\n",
    "# 2. CohereRerank (see https://python.langchain.com/docs/integrations/retrievers/cohere-reranker/)\n",
    "# from langchain_cohere import CohereRerank\n",
    "# reranker = CohereRerank(top_n=3, model=\"rerank-english-v2.0\")\n",
    "\n",
    "# Create combined retriever\n",
    "embedding_bm25_retriever_rerank = EmbeddingBM25RerankerRetriever(\n",
    "    vector_retriever=embedding_retriever,\n",
    "    bm25_retriever=bm25_retriever,\n",
    "    reranker=reranker,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8b93626005638acd",
   "metadata": {},
   "source": "### Contextual Retrievers"
  },
  {
   "cell_type": "code",
   "id": "9b9ee0db80ba3e10",
   "metadata": {},
   "source": [
    "import tqdm as tqdm # For progress reporting\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "def create_contextual_chunks(chunks_):\n",
    "    print(f\"Creating {len(chunks_)} contextual chunks\")\n",
    "    \n",
    "    # uses a llm to add context to each chunk given the prompts defined above\n",
    "    contextual_documents = []\n",
    "    for chunk in tqdm.tqdm(chunks_):\n",
    "        context = prompt_document.format(WHOLE_DOCUMENT=WHOLE_DOCUMENT)\n",
    "        chunk_context = prompt_chunk.format(CHUNK_CONTENT=chunk)\n",
    "        llm_response = llm.invoke(context + chunk_context).content\n",
    "        page_content = f\"\"\"Text: {chunk.page_content}\\n\\n\\nContext: {llm_response}\"\"\"\n",
    "        doc = Document(page_content=page_content, metadata=chunk.metadata)\n",
    "        contextual_documents.append(doc)\n",
    "        print(f\"Created chunk - size: {len(page_content)}, llm_response: {llm_response}\")\n",
    "    return contextual_documents\n",
    "\n",
    "\n",
    "contextual_documents = create_contextual_chunks(chunks)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e73cc0678c5864af",
   "metadata": {},
   "source": [
    "print(contextual_documents[1].page_content, \"------------\", chunks[1].page_content)\n",
    "print(f\"Doc count {len(contextual_documents)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6ac021069406db1",
   "metadata": {},
   "source": [
    "contextual_embedding_retriever = create_embedding_retriever(contextual_documents)\n",
    "\n",
    "contextual_bm25_retriever = create_bm25_retriever(contextual_documents)\n",
    "\n",
    "contextual_embedding_bm25_retriever_rerank = EmbeddingBM25RerankerRetriever(\n",
    "    vector_retriever=contextual_embedding_retriever,\n",
    "    bm25_retriever=contextual_bm25_retriever,\n",
    "    reranker=reranker,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2b04b648230def7d",
   "metadata": {},
   "source": "## Generate Question-Context pairs"
  },
  {
   "cell_type": "code",
   "id": "a1373b118f3cea15",
   "metadata": {},
   "source": [
    "import json\n",
    "import re\n",
    "import uuid\n",
    "import warnings\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Prompt to generate questions\n",
    "DEFAULT_QA_GENERATE_PROMPT_TMPL = \"\"\"\\\n",
    "Context information is below.\n",
    "\n",
    "---------------------\n",
    "{context_str}\n",
    "---------------------\n",
    "\n",
    "Given the context information and no prior knowledge.\n",
    "generate only questions based on the below query.\n",
    "\n",
    "You are a Teacher/ Professor. Your task is to setup \\\n",
    "{num_questions_per_chunk} questions for an upcoming \\\n",
    "quiz/examination. The questions should be diverse in nature \\\n",
    "across the document. Restrict the questions to the \\\n",
    "context information provided.\"\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class QuestionContextEvalDataset(BaseModel):\n",
    "    \"\"\"Embedding QA Dataset.\n",
    "    Args:\n",
    "        queries (Dict[str, str]): Dict id -> query.\n",
    "        corpus (Dict[str, str]): Dict id -> string.\n",
    "        relevant_docs (Dict[str, List[str]]): Dict query id -> list of doc ids.\n",
    "    \"\"\"\n",
    "\n",
    "    queries: Dict[str, str]  # dict id -> query\n",
    "    corpus: Dict[str, str]  # dict id -> string\n",
    "    relevant_docs: Dict[str, List[str]]  # query id -> list of doc ids\n",
    "    mode: str = \"text\"\n",
    "\n",
    "    @property\n",
    "    def query_docid_pairs(self) -> List[Tuple[str, List[str]]]:\n",
    "        \"\"\"Get query, relevant doc ids.\"\"\"\n",
    "        return [\n",
    "            (query, self.relevant_docs[query_id])\n",
    "            for query_id, query in self.queries.items()\n",
    "        ]\n",
    "\n",
    "    def save_json(self, path: str) -> None:\n",
    "        \"\"\"Save json.\"\"\"\n",
    "        with open(path, \"w\") as f:\n",
    "            json.dump(self.dict(), f, indent=4)\n",
    "\n",
    "    @classmethod\n",
    "    def from_json(cls, path: str) -> \"QuestionContextEvalDataset\":\n",
    "        \"\"\"Load json.\"\"\"\n",
    "        with open(path) as f:\n",
    "            data = json.load(f)\n",
    "        return cls(**data)\n",
    "\n",
    "\n",
    "def generate_question_context_pairs(\n",
    "    documents: List[Document],\n",
    "    llm,\n",
    "    qa_generate_prompt_tmpl: str = DEFAULT_QA_GENERATE_PROMPT_TMPL,\n",
    "    num_questions_per_chunk: int = 2,\n",
    ") -> QuestionContextEvalDataset:\n",
    "    \"\"\"Generate evaluation dataset using watsonx LLM and a set of chunks with their chunk_ids\n",
    "\n",
    "    Args:\n",
    "        documents (List[Document]): chunks of data with chunk_id\n",
    "        llm: LLM used for generating questions\n",
    "        qa_generate_prompt_tmpl (str): prompt template used for generating questions\n",
    "        num_questions_per_chunk (int): number of questions generated per chunk\n",
    "\n",
    "    Returns:\n",
    "        List[Documents]: List of langchain document objects with page content and metadata\n",
    "    \"\"\"\n",
    "    doc_dict = {doc.metadata[\"doc_id\"]: doc.page_content for doc in documents}\n",
    "    queries = {}\n",
    "    relevant_docs = {}\n",
    "    for doc_id, text in tqdm(doc_dict.items()):\n",
    "        query = qa_generate_prompt_tmpl.format(\n",
    "            context_str=text, num_questions_per_chunk=num_questions_per_chunk\n",
    "        )\n",
    "        response = llm.invoke(query).content\n",
    "        result = re.split(r\"\\n+\", response.strip())\n",
    "        print(result)\n",
    "        questions = [\n",
    "            re.sub(r\"^\\d+[\\).\\s]\", \"\", question).strip() for question in result\n",
    "        ]\n",
    "        questions = [question for question in questions if len(question) > 0][\n",
    "            :num_questions_per_chunk\n",
    "        ]\n",
    "\n",
    "        num_questions_generated = len(questions)\n",
    "        if num_questions_generated < num_questions_per_chunk:\n",
    "            warnings.warn(\n",
    "                f\"Fewer questions generated ({num_questions_generated}) \"\n",
    "                f\"than requested ({num_questions_per_chunk}).\"\n",
    "            )\n",
    "        for question in questions:\n",
    "            question_id = str(uuid.uuid4())\n",
    "            queries[question_id] = question\n",
    "            relevant_docs[question_id] = [doc_id]\n",
    "    # construct dataset\n",
    "    return QuestionContextEvalDataset(\n",
    "        queries=queries, corpus=doc_dict, relevant_docs=relevant_docs\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "49f7a07d9c8a192c",
   "metadata": {},
   "source": "qa_pairs = generate_question_context_pairs(chunks, llm, num_questions_per_chunk=2)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "45bbeb3ef1c0c45e",
   "metadata": {},
   "source": "## Evaluate"
  },
  {
   "cell_type": "code",
   "id": "11c7abff478ba921",
   "metadata": {},
   "source": [
    "def compute_hit_rate(expected_ids, retrieved_ids):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    expected_ids List[str]: The ground truth doc_id\n",
    "    retrieved_ids List[str]: The doc_id from retrieved chunks\n",
    "\n",
    "    Returns:\n",
    "        float: hit rate as a decimal\n",
    "    \"\"\"\n",
    "    if retrieved_ids is None or expected_ids is None:\n",
    "        raise ValueError(\"Retrieved ids and expected ids must be provided\")\n",
    "    is_hit = any(id in expected_ids for id in retrieved_ids)\n",
    "    return 1.0 if is_hit else 0.0\n",
    "\n",
    "\n",
    "def compute_mrr(expected_ids, retrieved_ids):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    expected_ids List[str]: The ground truth doc_id\n",
    "    retrieved_ids List[str]: The doc_id from retrieved chunks\n",
    "\n",
    "    Returns:\n",
    "        float: MRR score as a decimal\n",
    "    \"\"\"\n",
    "    if retrieved_ids is None or expected_ids is None:\n",
    "        raise ValueError(\"Retrieved ids and expected ids must be provided\")\n",
    "    for i, id in enumerate(retrieved_ids):\n",
    "        if id in expected_ids:\n",
    "            return 1.0 / (i + 1)\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "def compute_ndcg(expected_ids, retrieved_ids):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    expected_ids List[str]: The ground truth doc_id\n",
    "    retrieved_ids List[str]: The doc_id from retrieved chunks\n",
    "\n",
    "    Returns:\n",
    "        float: nDCG score as a decimal\n",
    "    \"\"\"\n",
    "    if retrieved_ids is None or expected_ids is None:\n",
    "        raise ValueError(\"Retrieved ids and expected ids must be provided\")\n",
    "    dcg = 0.0\n",
    "    idcg = 0.0\n",
    "    for i, id in enumerate(retrieved_ids):\n",
    "        if id in expected_ids:\n",
    "            dcg += 1.0 / (i + 1)\n",
    "        idcg += 1.0 / (i + 1)\n",
    "    return dcg / idcg"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "21527aa54b2317d",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def extract_queries(dataset):\n",
    "    values = []\n",
    "    for value in dataset.queries.values():\n",
    "        values.append(value)\n",
    "    return values\n",
    "\n",
    "\n",
    "def extract_doc_ids(documents_):\n",
    "    doc_ids = []\n",
    "    for doc in documents_:\n",
    "        doc_ids.append(f\"{doc.metadata['doc_id']}\")\n",
    "    return doc_ids\n",
    "\n",
    "\n",
    "def evaluate(retriever, dataset):\n",
    "    mrr_result = []\n",
    "    hit_rate_result = []\n",
    "    ndcg_result = []\n",
    "\n",
    "    # Loop over dataset\n",
    "    for i in tqdm(range(len(dataset.queries))):\n",
    "        context = retriever.invoke(extract_queries(dataset)[i])\n",
    "\n",
    "        expected_ids = dataset.relevant_docs[list(dataset.queries.keys())[i]]\n",
    "        retrieved_ids = extract_doc_ids(context)\n",
    "        # compute metrics\n",
    "        mrr = compute_mrr(expected_ids=expected_ids, retrieved_ids=retrieved_ids)\n",
    "        hit_rate = compute_hit_rate(\n",
    "            expected_ids=expected_ids, retrieved_ids=retrieved_ids\n",
    "        )\n",
    "        ndgc = compute_ndcg(expected_ids=expected_ids, retrieved_ids=retrieved_ids)\n",
    "        # append results\n",
    "        mrr_result.append(mrr)\n",
    "        hit_rate_result.append(hit_rate)\n",
    "        ndcg_result.append(ndgc)\n",
    "\n",
    "    array2D = np.array([mrr_result, hit_rate_result, ndcg_result])\n",
    "    mean_results = np.mean(array2D, axis=1)\n",
    "    results_df = pd.DataFrame(mean_results)\n",
    "    results_df.index = [\"MRR\", \"Hit Rate\", \"nDCG\"]\n",
    "    return results_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9f9c76d4f00fc8b",
   "metadata": {},
   "source": [
    "embedding_bm25_rerank_results = evaluate(embedding_bm25_retriever_rerank, qa_pairs)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "48446a2a806329db",
   "metadata": {},
   "source": [
    "contextual_embedding_bm25_rerank_results = evaluate(\n",
    "    contextual_embedding_bm25_retriever_rerank, qa_pairs\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5f15b680db1e2a4c",
   "metadata": {},
   "source": [
    "embedding_retriever_results = evaluate(embedding_retriever, qa_pairs)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9abc2f5386eea350",
   "metadata": {},
   "source": [
    "contextual_embedding_retriever_results = evaluate(\n",
    "    contextual_embedding_retriever, qa_pairs\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "21a7886c219437f2",
   "metadata": {},
   "source": [
    "bm25_results = evaluate(bm25_retriever, qa_pairs)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "226c01c4fb0441e8",
   "metadata": {},
   "source": [
    "contextual_bm25_results = evaluate(contextual_bm25_retriever, qa_pairs)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b0f932b2b804e38a",
   "metadata": {},
   "source": [
    "def display_results(name, eval_results):\n",
    "    \"\"\"Display results from evaluate.\"\"\"\n",
    "\n",
    "    metrics = [\"MRR\", \"Hit Rate\", \"nDCG\"]\n",
    "\n",
    "    columns = {\n",
    "        \"Retrievers\": [name],\n",
    "        **{metric: val for metric, val in zip(metrics, eval_results.values)},\n",
    "    }\n",
    "\n",
    "    metric_df = pd.DataFrame(columns)\n",
    "\n",
    "    return metric_df\n",
    "\n",
    "\n",
    "pd.concat(\n",
    "    [\n",
    "        display_results(\"Embedding Retriever\", embedding_retriever_results),\n",
    "        display_results(\"BM25 Retriever\", bm25_results),\n",
    "        display_results(\n",
    "            \"Embedding + BM25 Retriever + Reranker\",\n",
    "            embedding_bm25_rerank_results,\n",
    "        ),\n",
    "    ],\n",
    "    ignore_index=True,\n",
    "    axis=0,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ede2c131b792589b",
   "metadata": {},
   "source": [
    "pd.concat(\n",
    "    [\n",
    "        display_results(\n",
    "            \"Contextual Embedding Retriever\", contextual_embedding_retriever_results\n",
    "        ),\n",
    "        display_results(\"Contextual BM25 Retriever\", contextual_bm25_results),\n",
    "        display_results(\n",
    "            \"Contextual Embedding + BM25 Retriever + Reranker\",\n",
    "            contextual_embedding_bm25_rerank_results,\n",
    "        ),\n",
    "    ],\n",
    "    ignore_index=True,\n",
    "    axis=0,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
