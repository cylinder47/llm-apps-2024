{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3.3 Contextual compression\n",
   "id": "3b48e75bc84e1318"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Setup\n",
    "\n",
    "### Install dependencies"
   ],
   "id": "4b94a37c5d924654"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%pip install python-dotenv~=1.0 docarray~=0.40.0 pypdf~=5.1 --upgrade --quiet\n",
    "%pip install chromadb~=0.5.18 sentence-transformers~=3.3 --upgrade --quiet \n",
    "%pip install langchain~=0.3.7 langchain_openai~=0.2.6 langchain_community~=0.3.5 langchain-chroma~=0.1.4 langchainhub~=0.1.21 --upgrade --quiet\n",
    "\n",
    "# If running locally, you can do this instead:\n",
    "#%pip install -r ../requirements.txt"
   ],
   "id": "24bc56675ca819",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load environment variables",
   "id": "118972a0260de649"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# If running in Google Colab, you can use this code instead:\n",
    "# from google.colab import userdata\n",
    "# os.environ[\"AZURE_OPENAI_API_KEY\"] = userdata.get(\"AZURE_OPENAI_API_KEY\")\n",
    "# os.environ[\"AZURE_OPENAI_ENDPOINT\"] = userdata.get(\"AZURE_OPENAI_ENDPOINT\")"
   ],
   "id": "60c6d2dcb069aa5e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setup models",
   "id": "b6bb0e453051a190"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\n",
    "api_version = \"2024-10-01-preview\"\n",
    "llm = AzureChatOpenAI(deployment_name=\"gpt-4o\", temperature=0.0, openai_api_version=api_version)\n",
    "embedding_model = AzureOpenAIEmbeddings(model=\"text-embedding-3-large\", openai_api_version=api_version)"
   ],
   "id": "c3135eb3753ad70a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setup path to data ",
   "id": "12729200b12dc420"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "data_path = \"../data\"",
   "id": "af63a1ea28634be3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setup LangSmith tracing for this notebook",
   "id": "e069caa159bfbb6c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "# API key etc is in the .env file\n",
    "# my_name = \"Totoro\"\n",
    "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "# os.environ[\"LANGCHAIN_PROJECT\"] = f\"tokyo24-test-{my_name}\""
   ],
   "id": "37da02c98c1d4600",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Let's setup our vectorDB as before\n",
    "Load ML sample docs and setup Vector DB"
   ],
   "id": "400dff8193a83e5d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load PDFs\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "loaders = [\n",
    "    PyPDFLoader(f\"{data_path}/MachineLearning-Lecture01.pdf\"),\n",
    "    PyPDFLoader(f\"{data_path}/MachineLearning-Lecture01.pdf\"),\n",
    "    PyPDFLoader(f\"{data_path}/MachineLearning-Lecture03.pdf\")\n",
    "]\n",
    "docs = []\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load())\n",
    "\n",
    "# Split\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1500,\n",
    "    chunk_overlap = 150\n",
    ")\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Setup vector DB\n",
    "from langchain.vectorstores import Chroma\n",
    "persist_directory = './db/chroma-ML-docs/'\n",
    "vectordb = Chroma.from_documents(\n",
    "    collection_name=\"ml_docs\",\n",
    "    documents=splits,\n",
    "    embedding=embedding_model,\n",
    "    #persist_directory=persist_directory # Optionally persist the database\n",
    ")\n",
    "\n",
    "print(vectordb._collection.count())"
   ],
   "id": "3c1fef75d278c75a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## What is compression?\n",
    "\n",
    "Another approach for improving the quality of retrieved docs is compression.\n",
    "\n",
    "Information most relevant to a query may be buried in a document with a lot of irrelevant text. \n",
    "\n",
    "Passing that full document through your application can lead to more expensive LLM calls and poorer responses.\n",
    "\n",
    "Contextual compression is meant to fix this. We start by looking at a solution that uses an **LLM** for extracting content relevant to the query: [**LLMChainExtractor**](https://python.langchain.com/docs/how_to/contextual_compression/#adding-contextual-compression-with-an-llmchainextractor) "
   ],
   "id": "61efdb0b72b8e5f7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor"
   ],
   "id": "f0e8b7ae29fac0b5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Just making output a bit nicer\n",
    "def pretty_print_docs(docs):\n",
    "    print(f\"\\n{'-' * 100}\\n\".join([f\"Document {i+1} ({len(d.page_content)}):\\n\\n\" + d.page_content for i, d in enumerate(docs)]))"
   ],
   "id": "6da7d3e748c487db",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Wrap our vectorstore\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=vectordb.as_retriever()\n",
    ")"
   ],
   "id": "ce7689aa33f24d75",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "question = \"what did they say about matlab?\"\n",
    "compressed_docs = compression_retriever.invoke(question)\n",
    "pretty_print_docs(compressed_docs)\n",
    "print(f\"No. of docs used: {len(compressed_docs)}\")"
   ],
   "id": "6869efb1663d719d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### EXERCISE - try another compressors (see link below)\n",
    "https://python.langchain.com/docs/how_to/contextual_compression/#more-built-in-compressors-filters\n",
    "\n",
    "#### Try for instance:\n",
    " - [**LLMChainFilter**](https://python.langchain.com/docs/how_to/contextual_compression/#llmchainfilter) - slightly simpler but more robust LLM-based solution\n",
    " - [**LLMListwiseRerank**](https://python.langchain.com/docs/how_to/contextual_compression/#llmlistwisererank) - using a zero-shot listwise document reranking proposed in this [paper](https://arxiv.org/pdf/2305.02156) \n",
    " - [**EmbeddingsFilter**](https://python.langchain.com/docs/how_to/contextual_compression/#embeddingsfilter) - using embeddings for faster/cheaper results"
   ],
   "id": "f685a7f37758f39f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Combining various techniques\n",
    "\n",
    "Combining compression and MMR can lead to even better results."
   ],
   "id": "48b74b51ae6cbf80"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=vectordb.as_retriever(search_type = \"mmr\")\n",
    ")"
   ],
   "id": "3b114d480b811523",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "question = \"what did they say about matlab?\"\n",
    "compressed_docs = compression_retriever.invoke(question)\n",
    "pretty_print_docs(compressed_docs)\n",
    "print(f\"No. of docs used: {len(compressed_docs)}\")"
   ],
   "id": "c08a4e9cc9c75cb9",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
