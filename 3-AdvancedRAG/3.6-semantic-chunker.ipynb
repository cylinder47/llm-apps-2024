{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3.5 Semantic chunker\n",
   "id": "55c16125f02100af"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Setup\n",
    "\n",
    "### Install dependencies"
   ],
   "id": "9a9fab40957ed590"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%pip install python-dotenv~=1.0 docarray~=0.40.0 pypdf~=5.1 --upgrade --quiet\n",
    "%pip install chromadb~=0.5.18 sentence-transformers~=3.3 --upgrade --quiet \n",
    "%pip install langchain~=0.3.7 langchain_openai~=0.2.6 langchain_community~=0.3.5 langchain-chroma~=0.1.4 langchainhub~=0.1.21 --upgrade --quiet\n",
    "%pip install langchain_experimental~=0.3.3 --upgrade --quiet\n",
    "\n",
    "# If running locally, you can do this instead:\n",
    "#%pip install -r ../requirements.txt"
   ],
   "id": "57a539e4cb83fece",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load environment variables",
   "id": "c40c24c3815d3c62"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# If running in Google Colab, you can use this code instead:\n",
    "# from google.colab import userdata\n",
    "# os.environ[\"AZURE_OPENAI_API_KEY\"] = userdata.get(\"AZURE_OPENAI_API_KEY\")\n",
    "# os.environ[\"AZURE_OPENAI_ENDPOINT\"] = userdata.get(\"AZURE_OPENAI_ENDPOINT\")"
   ],
   "id": "bcd2f7b0f2b8c5ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setup models",
   "id": "16af2e2b91c2accb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\n",
    "api_version = \"2024-10-01-preview\"\n",
    "llm = AzureChatOpenAI(deployment_name=\"gpt-4o\", temperature=0.0, api_version=api_version)\n",
    "embedding_model = AzureOpenAIEmbeddings(model=\"text-embedding-3-large\", api_version=api_version)"
   ],
   "id": "58faa7fc3dc01629",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setup LangSmith tracing for this notebook",
   "id": "1898dde731563381"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "# API key etc is in the .env file\n",
    "# my_name = \"Totoro\"\n",
    "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "# os.environ[\"LANGCHAIN_PROJECT\"] = f\"tokyo24-test-{my_name}\""
   ],
   "id": "6f2ca331bd1d621e",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c3ee8d00",
   "metadata": {},
   "source": [
    "# How to split text based on semantic similarity\n",
    "\n",
    "Taken from Greg Kamradt's wonderful notebook:\n",
    "[**5_Levels_Of_Text_Splitting**](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb)\n",
    "\n",
    "All credit to him.\n",
    "\n",
    "This guide covers how to split chunks based on their semantic similarity. If embeddings are sufficiently far apart, chunks are split.\n",
    "\n",
    "At a high level, this splits into sentences, then groups into groups of 3\n",
    "sentences, and then merges one that are similar in the embedding space.\n",
    "\n",
    "### Some benefits of this approach are:\n",
    "1.\tEnhanced Retrieval Accuracy: By segmenting documents into semantically coherent chunks, retrieval systems can more effectively identify and extract relevant information, leading to more precise responses.\n",
    "2.\tImproved Context Preservation: Semantic chunking ensures that each segment maintains its contextual integrity, preventing the disruption of ideas that can occur with fixed-size chunking methods.\n",
    "3.\tReduced Hallucinations: By focusing on meaningful segments, semantic chunking allows for more efficient indexing and retrieval, optimizing computational resources and improving response times. \n",
    "\n",
    "### Three breakpoint types are available for semantic splitting:\n",
    "   - 'percentile': Splits at differences greater than the X percentile.\n",
    "   - 'standard_deviation': Splits at differences greater than X standard deviations.\n",
    "   - 'interquartile': Uses the interquartile distance to determine split points.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20cdf54",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-11-14T14:25:44.626146Z"
    }
   },
   "source": [
    "## Load Example Data"
   ]
  },
  {
   "cell_type": "code",
   "id": "313fb032",
   "metadata": {},
   "source": [
    "# This is a long document we can split up.\n",
    "with open(\"../data/state_of_the_union.txt\") as f:\n",
    "    state_of_the_union = f.read()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f7436e15",
   "metadata": {},
   "source": [
    "## Create Text Splitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774a5199-c2ff-43bc-bf07-87573e0b8db4",
   "metadata": {},
   "source": [
    "To instantiate a [SemanticChunker](https://python.langchain.com/api_reference/experimental/text_splitter/langchain_experimental.text_splitter.SemanticChunker.html), we must specify an embedding model. Below we will use [OpenAIEmbeddings](https://python.langchain.com/api_reference/community/embeddings/langchain_community.embeddings.openai.OpenAIEmbeddings.html). "
   ]
  },
  {
   "cell_type": "code",
   "id": "a88ff70c",
   "metadata": {},
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "\n",
    "text_splitter = SemanticChunker(embedding_model)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "91b14834",
   "metadata": {},
   "source": [
    "## Split Text\n",
    "\n",
    "We split text in the usual way, e.g., by invoking `.create_documents` to create LangChain [Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html) objects:"
   ]
  },
  {
   "cell_type": "code",
   "id": "295ec095",
   "metadata": {},
   "source": [
    "docs = text_splitter.create_documents([state_of_the_union])\n",
    "print(docs[0].page_content)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9aed73b2",
   "metadata": {},
   "source": [
    "## Breakpoints\n",
    "\n",
    "This chunker works by determining when to \"break\" apart sentences. This is done by looking for differences in embeddings between any two sentences. When that difference is past some threshold, then they are split.\n",
    "\n",
    "There are a few ways to determine what that threshold is, which are controlled by the `breakpoint_threshold_type` kwarg.\n",
    "\n",
    "### Percentile\n",
    "\n",
    "The default way to split is based on percentile. In this method, all differences between sentences are calculated, and then any difference greater than the X percentile is split."
   ]
  },
  {
   "cell_type": "code",
   "id": "a9a3b9cd",
   "metadata": {},
   "source": [
    "text_splitter = SemanticChunker(\n",
    "    embedding_model, breakpoint_threshold_type=\"percentile\"\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f311e67e",
   "metadata": {},
   "source": [
    "docs = text_splitter.create_documents([state_of_the_union])\n",
    "print(docs[0].page_content)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5f5930de",
   "metadata": {},
   "source": [
    "print(len(docs))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b6b51104",
   "metadata": {},
   "source": [
    "### Standard Deviation\n",
    "\n",
    "In this method, any difference greater than X standard deviations is split."
   ]
  },
  {
   "cell_type": "code",
   "id": "ff5e005c",
   "metadata": {},
   "source": [
    "text_splitter = SemanticChunker(\n",
    "    embedding_model, breakpoint_threshold_type=\"standard_deviation\"\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "01b8ffc0",
   "metadata": {},
   "source": [
    "docs = text_splitter.create_documents([state_of_the_union])\n",
    "print(docs[0].page_content)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8938a5e3",
   "metadata": {},
   "source": [
    "print(len(docs))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6897261f",
   "metadata": {},
   "source": [
    "### Interquartile\n",
    "\n",
    "In this method, the interquartile distance is used to split chunks."
   ]
  },
  {
   "cell_type": "code",
   "id": "8977355b",
   "metadata": {},
   "source": [
    "text_splitter = SemanticChunker(\n",
    "    embedding_model, breakpoint_threshold_type=\"interquartile\"\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "59a40364",
   "metadata": {},
   "source": [
    "docs = text_splitter.create_documents([state_of_the_union])\n",
    "print(docs[0].page_content)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3a0db107",
   "metadata": {},
   "source": [
    "print(len(docs))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "423c6e099e94ca69",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Gradient\n",
    "\n",
    "In this method, the gradient of distance is used to split chunks along with the percentile method.\n",
    "This method is useful when chunks are highly correlated with each other or specific to a domain e.g. legal or medical. The idea is to apply anomaly detection on gradient array so that the distribution become wider and easy to identify boundaries in highly semantic data."
   ]
  },
  {
   "cell_type": "code",
   "id": "b1f65472",
   "metadata": {},
   "source": [
    "text_splitter = SemanticChunker(\n",
    "    embedding_model, breakpoint_threshold_type=\"gradient\"\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e9f393d316ce1f6c",
   "metadata": {},
   "source": [
    "docs = text_splitter.create_documents([state_of_the_union])\n",
    "print(docs[0].page_content)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a407cd57f02a0db4",
   "metadata": {},
   "source": [
    "print(len(docs))"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
