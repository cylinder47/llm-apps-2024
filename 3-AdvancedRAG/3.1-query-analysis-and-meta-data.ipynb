{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3.1 Advanced RAG intro - query analysis and meta-data\n",
   "id": "81c2ce3742ccdccb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Setup\n",
    "\n",
    "### Install dependencies"
   ],
   "id": "8863b270dbdeceb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%pip install python-dotenv~=1.0 docarray~=0.40.0 pypdf~=5.1 --upgrade --quiet\n",
    "%pip install chromadb~=0.5.18 sentence-transformers~=3.3 --upgrade --quiet \n",
    "%pip install langchain~=0.3.7 langchain_openai~=0.2.6 langchain_community~=0.3.5 langchain-chroma~=0.1.4 langchainhub~=0.1.21 --upgrade --quiet\n",
    "\n",
    "# If running locally, you can do this instead:\n",
    "#%pip install -r ../requirements.txt"
   ],
   "id": "cdb7524edadc7d97",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load environment variables",
   "id": "9167196e4bd4669f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# If running in Google Colab, you can use this code instead:\n",
    "# from google.colab import userdata\n",
    "# os.environ[\"AZURE_OPENAI_API_KEY\"] = userdata.get(\"AZURE_OPENAI_API_KEY\")\n",
    "# os.environ[\"AZURE_OPENAI_ENDPOINT\"] = userdata.get(\"AZURE_OPENAI_ENDPOINT\")"
   ],
   "id": "be489920b631789d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setup models",
   "id": "8dbfb71250edea80"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\n",
    "api_version = \"2024-10-01-preview\"\n",
    "llm = AzureChatOpenAI(deployment_name=\"gpt-4o\", temperature=0.0, openai_api_version=api_version)\n",
    "embedding_model = AzureOpenAIEmbeddings(model=\"text-embedding-3-large\", openai_api_version=api_version)"
   ],
   "id": "36a32dc38f9ac0d3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setup LangSmith tracing for this notebook",
   "id": "93340862e2903b4d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "# API key etc is in the .env file\n",
    "# my_name = \"Totoro\"\n",
    "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "# os.environ[\"LANGCHAIN_PROJECT\"] = f\"tokyo24-test-{my_name}\""
   ],
   "id": "1543c27220eddad5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setup path to data ",
   "id": "9ed5dbc63b0f2f5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "data_path = \"../data\"",
   "id": "832992966b7aa3c6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Let's setup our vectorDB as before\n",
    "Load ML sample docs and setup Vector DB"
   ],
   "id": "28be7e5be25a3059"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load PDFs\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "loaders = [\n",
    "    PyPDFLoader(f\"{data_path}/MachineLearning-Lecture01.pdf\"),\n",
    "    PyPDFLoader(f\"{data_path}/MachineLearning-Lecture01.pdf\"),\n",
    "    PyPDFLoader(f\"{data_path}/MachineLearning-Lecture03.pdf\")\n",
    "]\n",
    "docs = []\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load())\n",
    "\n",
    "# Split\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1500,\n",
    "    chunk_overlap = 150\n",
    ")\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Setup vector DB\n",
    "from langchain.vectorstores import Chroma\n",
    "persist_directory = './db/chroma-ML-docs/'\n",
    "vectordb = Chroma.from_documents(\n",
    "    collection_name=\"ml_docs\",\n",
    "    documents=splits,\n",
    "    embedding=embedding_model,\n",
    "    #persist_directory=persist_directory # Optionally persist the database\n",
    ")\n",
    "\n",
    "print(vectordb._collection.count())"
   ],
   "id": "1bc6a27277cbca20",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Query analysis - understanding what the user is asking for\n",
    "\n",
    "### Let's start by setting up a simple model\n",
    "The model just hols a simple flag, indicating if a question is related to the topics of the documents in the database. "
   ],
   "id": "4f07cb4def1852a0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class QueryAnalysis(BaseModel):\n",
    "    \"\"\"Binary score for relevance of the user's question to knowledge base topics.\"\"\"\n",
    "\n",
    "    question_relevant_to_topics: bool = Field(description=\"User question is related to the topics in the knowledge base, 'true' or 'false'\")"
   ],
   "id": "a1a5797db06ea688",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## We need a matching promt ",
   "id": "5872168122713ed9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Prompt \n",
    "system = \"\"\"You are an expert determining if a user question is related to topics of data stored in a knowledge base.\n",
    "    If the question contains keyword(s) or semantic meaning related to the topics, mark it as relevant to the topics. \\n\n",
    "    Give a binary value of 'true' or 'false' to indicate whether the question is relevant. \\n\\n \n",
    "    The topics of the data stored in the knowledge base are: \\n{document_topics}\"\"\"\n",
    "analysis_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")"
   ],
   "id": "5b7ddcc70a389b34",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Now we can construct a chain that uses structured output",
   "id": "1fd72d5442edbe64"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# TODO: Your task - create the chain for query analysis\n",
    "query_analysis_chain = analysis_prompt | llm.with_structured_output(QueryAnalysis)"
   ],
   "id": "ccd4a319d9ba2566",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "topics = \"Machine Learning, Math, Computer Science, CS229, Andrew Ng\"\n",
    "question = \"what did they say about matlab?\"\n",
    "#question = \"what must I see in Tokyo?\"\n",
    "response = query_analysis_chain.invoke({\"question\": question, \"document_topics\": topics})\n",
    "print(type(response))"
   ],
   "id": "3cc85f672e952a14",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "analysis_response: QueryAnalysis = response\n",
    "print(f\"Relevant: {analysis_response.question_relevant_to_topics}\")"
   ],
   "id": "4438dafa8b62be4c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Addressing Specificity: working with metadata\n",
    "\n",
    "In last lecture, we showed that a question about the third lecture can include results from other lectures as well.\n",
    "\n",
    "To address this, many vectorstores support operations on `metadata`.\n",
    "\n",
    "`metadata` provides context for each embedded chunk."
   ],
   "id": "bf507bf799252daf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "question = \"what did they say about regression in the third lecture?\"",
   "id": "e07bf76be135252b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "docs = vectordb.similarity_search(\n",
    "    question,\n",
    "    k=3,\n",
    "    filter={\"source\":\"../data/MachineLearning-Lecture03.pdf\"}\n",
    ")"
   ],
   "id": "8e2c00dac0a3454d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for d in docs:\n",
    "    print(d.metadata)"
   ],
   "id": "2b46ca99a232dd04",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Addressing Specificity: working with metadata using self-query retriever\n",
    "\n",
    "But we have an interesting challenge: we often want to infer the metadata from the query itself.\n",
    "\n",
    "To address this, we can use `SelfQueryRetriever`, which uses an LLM to extract:\n",
    " \n",
    "1. The `query` string to use for vector search\n",
    "2. A metadata filter to pass in as well\n",
    "\n",
    "Most vector databases support metadata filters, so this doesn't require any new databases or indexes."
   ],
   "id": "198b3bb910c051e2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain.chains.query_constructor.base import AttributeInfo"
   ],
   "id": "e87259499daab7c3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain.storage import InMemoryStore\n",
    "store = InMemoryStore()\n",
    "\n",
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"source\",\n",
    "        description=\"The lecture the chunk is from, should be one of `../data/MachineLearning-Lecture01.pdf`, `../data/MachineLearning-Lecture02.pdf`, or `../data/MachineLearning-Lecture03.pdf`\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"page\",\n",
    "        description=\"The page from the lecture\",\n",
    "        type=\"integer\",\n",
    "    ),\n",
    "]"
   ],
   "id": "b0b3304574cc4126",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "document_content_description = \"Lecture notes\"\n",
    "\n",
    "retriever = SelfQueryRetriever.from_llm(\n",
    "    llm,\n",
    "    vectordb,\n",
    "    document_content_description,\n",
    "    metadata_field_info,\n",
    "    verbose=True\n",
    ")"
   ],
   "id": "5f66c4444cd6c71b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "question = \"what did they say about regression in the third lecture?\"\n",
    "docs = retriever.invoke(question)\n",
    "\n",
    "# To see what's happening under the hood, you can use a ConsoleCallbackHandler:\n",
    "#from langchain.callbacks.tracers import ConsoleCallbackHandler\n",
    "#docs = retriever.with_config({'callbacks': [ConsoleCallbackHandler()]}).invoke(question)\n",
    " \n",
    "print(len(docs))"
   ],
   "id": "4c768a7ebad0f74f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for d in docs:\n",
    "    print(d.metadata)"
   ],
   "id": "15b7dd1e9064ed61",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n"
   ],
   "id": "86160c5f6412da37"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "472ee7510534329a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "e64afd567d56b001"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "9a44759d49cba542"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "#### Ok, some cheats"
   ],
   "id": "37db4bef096fd350"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Query analysis chain:",
   "id": "315f332a93cd501c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "analysis_prompt | llm.with_structured_output(QueryAnalysis)",
   "id": "73d3b4ef509cd2eb",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
