{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 4.8 Trimming and filtering\n",
    "\n",
    "Derived from [![LangChain Academy](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e9eba12c7b7688aa3dbb5e_LCA-badge-green.svg)](https://academy.langchain.com/courses/take/intro-to-langgraph/)"
   ],
   "id": "cb1c5e302b2ca574"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Setup\n",
    "\n",
    "### Install dependencies"
   ],
   "id": "53891348bb7962e5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T07:04:18.010937Z",
     "start_time": "2024-11-14T07:04:15.560289Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 1,
   "source": [
    "%%capture --no-stderr\n",
    "%pip install python-dotenv~=1.0 --upgrade --quiet\n",
    "%pip install langchain_core~=0.3.17 langchain_openai~=0.2.6 langchain_community~=0.3.5 --upgrade --quiet\n",
    "%pip install langgraph~=0.2.46 --upgrade --quiet\n",
    "\n",
    "# If running locally, you can do this instead:\n",
    "#%pip install -r ../requirements.txt"
   ],
   "id": "4604eaa8327a4559"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load environment variables",
   "id": "d15e7d7c3b73bd2b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T07:04:18.132049Z",
     "start_time": "2024-11-14T07:04:18.126514Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 2,
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# If running in Google Colab, you can use this code instead:\n",
    "# from google.colab import userdata\n",
    "# os.environ[\"AZURE_OPENAI_API_KEY\"] = userdata.get(\"AZURE_OPENAI_API_KEY\")\n",
    "# os.environ[\"AZURE_OPENAI_ENDPOINT\"] = userdata.get(\"AZURE_OPENAI_ENDPOINT\")"
   ],
   "id": "59bb97e11afeb509"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setup models",
   "id": "b3564eee1f3a3451"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T07:04:18.965651Z",
     "start_time": "2024-11-14T07:04:18.137922Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 3,
   "source": [
    "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\n",
    "api_version = \"2024-10-01-preview\"\n",
    "llm = AzureChatOpenAI(deployment_name=\"gpt-4o\", temperature=0.0, openai_api_version=api_version)"
   ],
   "id": "565a3c25addcb1f1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setup LangSmith tracing for this notebook",
   "id": "96ca549aa9d3c96d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T07:04:18.973544Z",
     "start_time": "2024-11-14T07:04:18.972155Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 4,
   "source": [
    "#import os\n",
    "\n",
    "# API key etc is in the .env file\n",
    "#my_name = \"Totoro\"\n",
    "#os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "#os.environ[\"LANGCHAIN_PROJECT\"] = f\"tokyo24-langgraph-{my_name}\""
   ],
   "id": "b7b50d451802198b"
  },
  {
   "cell_type": "markdown",
   "id": "c52ea2f9-03ff-4647-b782-46867ebed04e",
   "metadata": {},
   "source": [
    "# Filtering and trimming messages\n",
    "\n",
    "## Review\n",
    "\n",
    "Now, we have a deeper understanding of a few things: \n",
    "\n",
    "* How to customize the graph state schema\n",
    "* How to define custom state reducers\n",
    "* How to use multiple graph state schemas\n",
    "\n",
    "## Goals\n",
    "\n",
    "Now, we can start using these concepts with models in LangGraph!\n",
    " \n",
    "In the next few sessions, we'll build towards a chatbot that has long-term memory.\n",
    "\n",
    "Because our chatbot will use messages, let's first talk a bit more about advanced ways to work with messages in graph state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f3fc90-58b6-4f7f-897e-dddf6ae532c7",
   "metadata": {},
   "source": [
    "## Messages as state\n",
    "\n",
    "First, let's define some messages."
   ]
  },
  {
   "cell_type": "code",
   "id": "cf11a463-e27a-4a05-b41d-64882e38edca",
   "metadata": {},
   "source": [
    "from pprint import pprint\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "messages = [AIMessage(f\"So you said you were researching ocean mammals?\", name=\"Bot\")]\n",
    "messages.append(HumanMessage(f\"Yes, I know about whales. But what others should I learn about?\", name=\"Lance\"))\n",
    "\n",
    "for m in messages:\n",
    "    m.pretty_print()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b814adcb-6bf9-4b75-be11-e59f933fbd0c",
   "metadata": {},
   "source": [
    "Recall we can pass them to a chat model."
   ]
  },
  {
   "cell_type": "code",
   "id": "4712e288-e622-48a2-ad3f-a52f65f3ab08",
   "metadata": {},
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "llm.invoke(messages)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fbd1dab8-0af8-4621-8264-ce65065f76ec",
   "metadata": {},
   "source": [
    "We can run our chat model in a simple graph with `MessagesState`."
   ]
  },
  {
   "cell_type": "code",
   "id": "bbd8c39c-633b-4176-9cc6-8318e42bb5dd",
   "metadata": {},
   "source": [
    "from IPython.display import Image, display\n",
    "from langgraph.graph import MessagesState\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "# Node\n",
    "def chat_model_node(state: MessagesState):\n",
    "    return {\"messages\": llm.invoke(state[\"messages\"])}\n",
    "\n",
    "# Build graph\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"chat_model\", chat_model_node)\n",
    "builder.add_edge(START, \"chat_model\")\n",
    "builder.add_edge(\"chat_model\", END)\n",
    "graph = builder.compile()\n",
    "\n",
    "# View\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3a5a3e4a-ccfd-4d14-81f1-f0de6e11a1e4",
   "metadata": {},
   "source": [
    "output = graph.invoke({'messages': messages})\n",
    "for m in output['messages']:\n",
    "    m.pretty_print()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "34c33e63-1ef4-412d-bb10-6a1b9e5b35a7",
   "metadata": {},
   "source": [
    "## Reducer\n",
    "\n",
    "A practical challenge when working with messages is managing long-running conversations. \n",
    "\n",
    "Long-running conversations result in high token usage and latency if we are not careful, because we pass a growing list of messages to the model.\n",
    "\n",
    "We have a few ways to address this.\n",
    "\n",
    "First, recall the trick we saw using `RemoveMessage` and the `add_messages` reducer."
   ]
  },
  {
   "cell_type": "code",
   "id": "222c6bc5-bb0e-4a43-80f5-c8ec38d99f3a",
   "metadata": {},
   "source": [
    "from langchain_core.messages import RemoveMessage\n",
    "\n",
    "# Nodes\n",
    "def filter_messages(state: MessagesState):\n",
    "    # Delete all but the 2 most recent messages\n",
    "    delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"][:-2]]\n",
    "    return {\"messages\": delete_messages}\n",
    "\n",
    "def chat_model_node(state: MessagesState):    \n",
    "    return {\"messages\": [llm.invoke(state[\"messages\"])]}\n",
    "\n",
    "# Build graph\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"filter\", filter_messages)\n",
    "builder.add_node(\"chat_model\", chat_model_node)\n",
    "builder.add_edge(START, \"filter\")\n",
    "builder.add_edge(\"filter\", \"chat_model\")\n",
    "builder.add_edge(\"chat_model\", END)\n",
    "graph = builder.compile()\n",
    "\n",
    "# View\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "95a7c2cc-54ce-43e7-9a90-abf37827d709",
   "metadata": {},
   "source": [
    "# Message list with a preamble\n",
    "messages = [AIMessage(\"Hi.\", name=\"Bot\", id=\"1\")]\n",
    "messages.append(HumanMessage(\"Hi.\", name=\"Lance\", id=\"2\"))\n",
    "messages.append(AIMessage(\"So you said you were researching ocean mammals?\", name=\"Bot\", id=\"3\"))\n",
    "messages.append(HumanMessage(\"Yes, I know about whales. But what others should I learn about?\", name=\"Lance\", id=\"4\"))\n",
    "\n",
    "# Invoke\n",
    "output = graph.invoke({'messages': messages})\n",
    "for m in output['messages']:\n",
    "    m.pretty_print()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f506457d-014b-4fee-a684-e5edfb4b8f0d",
   "metadata": {},
   "source": [
    "## Filtering messages\n",
    "\n",
    "If you don't need or want to modify the graph state, you can just filter the messages you pass to the chat model.\n",
    "\n",
    "For example, just pass in a filtered list: `llm.invoke(messages[-1:])` to the model."
   ]
  },
  {
   "cell_type": "code",
   "id": "22d0b904-7cd6-486b-8948-105bee3d4683",
   "metadata": {},
   "source": [
    "# Node\n",
    "def chat_model_node(state: MessagesState):\n",
    "    return {\"messages\": [llm.invoke(state[\"messages\"][-1:])]}\n",
    "\n",
    "# Build graph\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"chat_model\", chat_model_node)\n",
    "builder.add_edge(START, \"chat_model\")\n",
    "builder.add_edge(\"chat_model\", END)\n",
    "graph = builder.compile()\n",
    "\n",
    "# View\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6f58c6fc-532f-418d-b70a-cfcb3307daf5",
   "metadata": {},
   "source": [
    "Let's take our existing list of messages, append the above LLM response, and append a follow-up question."
   ]
  },
  {
   "cell_type": "code",
   "id": "16956015-1dbe-4108-89b5-4209b68b51ca",
   "metadata": {},
   "source": [
    "messages.append(output['messages'][-1])\n",
    "messages.append(HumanMessage(f\"Tell me more about Narwhals!\", name=\"Lance\"))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "85563415-c085-46a8-a4ac-155df798c54e",
   "metadata": {},
   "source": [
    "for m in messages:\n",
    "    m.pretty_print()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "23349705-a059-47b5-9760-d8f64e687393",
   "metadata": {},
   "source": [
    "# Invoke, using message filtering\n",
    "output = graph.invoke({'messages': messages})\n",
    "for m in output['messages']:\n",
    "    m.pretty_print()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "42e1d8d2-e297-4d78-b54c-d12b3c866745",
   "metadata": {},
   "source": [
    "The state has all of the mesages.\n",
    "\n",
    "But, let's look at the LangSmith trace to see that the model invocation only uses the last message:\n",
    "\n",
    "https://smith.langchain.com/public/75aca3ce-ef19-4b92-94be-0178c7a660d9/r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc40d930-3c1f-47fe-8d2a-ce174873353c",
   "metadata": {},
   "source": [
    "## Trim messages\n",
    "\n",
    "Another approach is to [trim messages](https://python.langchain.com/v0.2/docs/how_to/trim_messages/#getting-the-last-max_tokens-tokens), based upon a set number of tokens. \n",
    "\n",
    "This restricts the message history to a specified number of tokens.\n",
    "\n",
    "While filtering only returns a post-hoc subset of the messages between agents, trimming restricts the number of tokens that a chat model can use to respond.\n",
    "\n",
    "See the `trim_messages` below."
   ]
  },
  {
   "cell_type": "code",
   "id": "2ff99b81-cf03-4cc2-b44f-44829a73e1fd",
   "metadata": {},
   "source": [
    "from langchain_core.messages import trim_messages\n",
    "\n",
    "# Node\n",
    "def chat_model_node(state: MessagesState):\n",
    "    messages = trim_messages(\n",
    "            state[\"messages\"],\n",
    "            max_tokens=100,\n",
    "            strategy=\"last\",\n",
    "            token_counter=ChatOpenAI(model=\"gpt-4o\"),\n",
    "            allow_partial=False,\n",
    "        )\n",
    "    return {\"messages\": [llm.invoke(messages)]}\n",
    "\n",
    "# Build graph\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"chat_model\", chat_model_node)\n",
    "builder.add_edge(START, \"chat_model\")\n",
    "builder.add_edge(\"chat_model\", END)\n",
    "graph = builder.compile()\n",
    "\n",
    "# View\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "24df63ac-da29-4874-b3df-7e390e97cc8a",
   "metadata": {},
   "source": [
    "messages.append(output['messages'][-1])\n",
    "messages.append(HumanMessage(f\"Tell me where Orcas live!\", name=\"Lance\"))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6d9d8971-c75c-43ca-a209-eb1d07b2ead0",
   "metadata": {},
   "source": [
    "# Example of trimming messages\n",
    "trim_messages(\n",
    "            messages,\n",
    "            max_tokens=100,\n",
    "            strategy=\"last\",\n",
    "            token_counter=ChatOpenAI(model=\"gpt-4o\"),\n",
    "            allow_partial=False\n",
    "        )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ed70a269-a869-4fa0-a1df-29736a432c51",
   "metadata": {},
   "source": [
    "# Invoke, using message trimming in the chat_model_node \n",
    "messages_out_trim = graph.invoke({'messages': messages})"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "38b3db67-380e-46b5-9a6a-20100ba52008",
   "metadata": {},
   "source": [
    "Let's look at the LangSmith trace to see the model invocation:\n",
    "\n",
    "https://smith.langchain.com/public/b153f7e9-f1a5-4d60-8074-f0d7ab5b42ef/r"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
