{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 2.7 RAG with sources\n",
    "### Using structured output to include sources in the answer\n",
    "* Based on https://python.langchain.com/docs/how_to/qa_sources\n",
    "* See also: https://python.langchain.com/docs/how_to/qa_citations/"
   ],
   "id": "943ea36a1082641e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Setup\n",
    "\n",
    "### Install dependencies"
   ],
   "id": "c9dd64db952da1d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%pip install python-dotenv~=1.0 docarray~=0.40.0 pypdf~=5.1 --upgrade --quiet\n",
    "%pip install chromadb~=0.5.18 sentence-transformers~=3.3 --upgrade --quiet \n",
    "%pip install langchain~=0.3.7 langchain_openai~=0.2.6 langchain_community~=0.3.5 langchain-chroma~=0.1.4 langchainhub~=0.1.21 --upgrade --quiet\n",
    "\n",
    "# If running locally, you can do this instead:\n",
    "#%pip install -r ../requirements.txt"
   ],
   "id": "5e800faf4e660819",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load environment variables",
   "id": "23db6d4042f86418"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# If running in Google Colab, you can use this code instead:\n",
    "# from google.colab import userdata\n",
    "# os.environ[\"AZURE_OPENAI_API_KEY\"] = userdata.get(\"AZURE_OPENAI_API_KEY\")\n",
    "# os.environ[\"AZURE_OPENAI_ENDPOINT\"] = userdata.get(\"AZURE_OPENAI_ENDPOINT\")"
   ],
   "id": "e9f00cf70a4e8556",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setup models",
   "id": "9c3d11e1151a7e80"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\n",
    "api_version = \"2024-10-01-preview\"\n",
    "llm = AzureChatOpenAI(deployment_name=\"gpt-4o\", temperature=0.0, api_version=api_version)\n",
    "embedding_model = AzureOpenAIEmbeddings(model=\"text-embedding-3-large\", api_version=api_version)"
   ],
   "id": "ba095904eb63506",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setup LangSmith tracing for this notebook",
   "id": "3e0dcb326c5b015b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "# API key etc is in the .env file\n",
    "# my_name = \"Totoro\"\n",
    "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "# os.environ[\"LANGCHAIN_PROJECT\"] = f\"tokyo24-test-{my_name}\""
   ],
   "id": "f3b906942dc243b7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Setting up a basic RAG chain to start us off\n",
   "id": "6bde3aec39acd7df"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import bs4\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 1. Load, chunk and index the contents of the blog to create a retriever.\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=embedding_model)\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "\n",
    "# 2. Incorporate the retriever into a question-answering chain.\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ],
   "id": "1bfd8e7e18c145ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(vectorstore._collection.count())\n",
    "\n",
    "result = rag_chain.invoke({\"input\": \"What is Task Decomposition?\"})\n",
    "print(result[\"answer\"])"
   ],
   "id": "58985b34dd50ba7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Customizing the chain to prepare for more advanced output",
   "id": "b40bd9f55fd7299f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "# This Runnable takes a dict with keys 'input' and 'context',\n",
    "# formats them into a prompt, and generates a response.\n",
    "rag_chain_from_docs = (\n",
    "        {\n",
    "            \"input\": lambda x: x[\"input\"],  # input query\n",
    "            \"context\": lambda x: format_docs(x[\"context\"]),  # context\n",
    "        }\n",
    "        | prompt  # format query and context into prompt\n",
    "        | llm  # generate response\n",
    "        | StrOutputParser()  # coerce to string\n",
    ")\n",
    "\n",
    "# Pass input query to retriever\n",
    "return_input = RunnableLambda(lambda x: x[\"input\"]) # For clarity, you can also just use a lambda directly\n",
    "retrieve_docs = return_input | retriever\n",
    "\n",
    "# Below, we chain `.assign` calls. This takes a dict and successively\n",
    "# adds keys-- \"context\" and \"answer\"-- where the value for each key\n",
    "# is determined by a Runnable. The Runnable operates on all existing\n",
    "# keys in the dict.\n",
    "chain = RunnablePassthrough.assign(context=retrieve_docs).assign(\n",
    "    answer=rag_chain_from_docs\n",
    ")\n",
    "\n",
    "chain.invoke({\"input\": \"What is Task Decomposition\"})"
   ],
   "id": "e830bd7347eb1ac0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Structure sources in model response\n",
    "\n",
    "#### See also chapter `2.4 (Extraction)` and `2.3 (Tagging)` for a recap on this.\n",
    "\n",
    "Because the above LCEL implementation is composed of Runnable primitives, it is straightforward to extend. Below, we make a simple change:\n",
    "\n",
    "1. We use the model's tool-calling features to generate structured output, consisting of an answer and list of sources. The schema for the response is represented in the AnswerWithSources, below. Note that there is **_two ways_** of implementing this - as a `TypedDict` (if you only need JSON) or as a Pydantic `BaseModel` (if you want an object).\n",
    "2. We remove the `StrOutputParser()`, as we expect dict output in this scenario."
   ],
   "id": "17a30b226bd6a0e1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Desired schema for response\n",
    "\n",
    "# OPTION 1 - Simple JSON schema, by using TypedDict  \n",
    "class AnswerWithSources1(TypedDict):\n",
    "    \"\"\"An answer to the question, with sources.\"\"\"\n",
    "\n",
    "    answer: str\n",
    "    sources: Annotated[\n",
    "        list[str],\n",
    "        ...,\n",
    "        \"List of sources (author + year) used to answer the question\",\n",
    "    ]\n",
    "\n",
    "# OPTION 2 - Pydantic object\n",
    "# TODO: Your task - implement a this class after having tried option 1\n",
    "# Hit: Look at `1.4 (Extraction)` and how a list of responses is used there\n",
    "class AnswerWithSources2(BaseModel):\n",
    "    \"\"\"An answer to the question, with sources.\"\"\"\n",
    "    # TODO\n",
    "\n",
    "\n",
    "\n",
    "# Our rag_chain_from_docs has the following changes:\n",
    "# - add `.with_structured_output` to the LLM;\n",
    "# - remove the output parser\n",
    "rag_chain_from_docs = (\n",
    "        {\n",
    "            \"input\": lambda x: x[\"input\"],\n",
    "            \"context\": lambda x: format_docs(x[\"context\"]),\n",
    "        }\n",
    "        | prompt\n",
    "        | llm.with_structured_output(AnswerWithSources1)\n",
    ")\n",
    "\n",
    "retrieve_docs = (lambda x: x[\"input\"]) | retriever\n",
    "\n",
    "chain2 = RunnablePassthrough.assign(context=retrieve_docs).assign(\n",
    "    answer=rag_chain_from_docs\n",
    ")\n",
    "\n",
    "response = chain2.invoke({\"input\": \"What is Chain of Thought?\"})\n",
    "structured_answer = response[\"answer\"]\n",
    "print(type(structured_answer))"
   ],
   "id": "14dd8cbfc0f92b1b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "structured_answer_json = structured_answer\n",
    "#structured_answer_json = structured_answer.model_dump()\n",
    "\n",
    "# Pretty print the event (can also use `print(json.dumps(structured_answer_json, indent=2))`)\n",
    "from IPython.display import JSON\n",
    "JSON(structured_answer_json)"
   ],
   "id": "6afc8551ef8c6c78",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
