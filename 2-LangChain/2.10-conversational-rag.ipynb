{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2.6 Conversational RAG - Adding chat history",
   "id": "943ea36a1082641e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Setup\n",
    "\n",
    "### Install dependencies"
   ],
   "id": "c9dd64db952da1d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%pip install python-dotenv~=1.0 docarray~=0.40.0 pypdf~=5.1 --upgrade --quiet\n",
    "%pip install chromadb~=0.5.18 sentence-transformers~=3.3 --upgrade --quiet \n",
    "%pip install langchain~=0.3.7 langchain_openai~=0.2.6 langchain_community~=0.3.5 --upgrade --quiet\n",
    "\n",
    "# If running locally, you can do this instead:\n",
    "#%pip install -r ../requirements.txt"
   ],
   "id": "5e800faf4e660819",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load environment variables",
   "id": "23db6d4042f86418"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# If running in Google Colab, you can use this code instead:\n",
    "# from google.colab import userdata\n",
    "# os.environ[\"AZURE_OPENAI_API_KEY\"] = userdata.get(\"AZURE_OPENAI_API_KEY\")\n",
    "# os.environ[\"AZURE_OPENAI_ENDPOINT\"] = userdata.get(\"AZURE_OPENAI_ENDPOINT\")"
   ],
   "id": "e9f00cf70a4e8556",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setup models",
   "id": "9c3d11e1151a7e80"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\n",
    "api_version = \"2024-10-01-preview\"\n",
    "llm = AzureChatOpenAI(deployment_name=\"gpt-4o\", temperature=0.0, openai_api_version=api_version)\n",
    "embedding_model = AzureOpenAIEmbeddings(model=\"text-embedding-3-large\", openai_api_version=api_version)"
   ],
   "id": "ba095904eb63506",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setup LangSmith tracing for this notebook",
   "id": "3e0dcb326c5b015b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "# API key etc is in the .env file\n",
    "# my_name = \"Totoro\"\n",
    "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "# os.environ[\"LANGCHAIN_PROJECT\"] = f\"tokyo24-test-{my_name}\""
   ],
   "id": "f3b906942dc243b7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setup path to data ",
   "id": "979a99375d945ac8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "data_path = \"../data\"",
   "id": "438411075ed796ba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Initialize VectorDB - like before",
   "id": "6bde3aec39acd7df"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's load our vectorDB and set it up like in previous chapters. _If you already have a persisted vectorDB, you can skip to \"Vector DB\" below._",
   "id": "f3bd1965176d16fc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# LOAD DOCS\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# Load PDFs\n",
    "loaders = [\n",
    "    PyPDFLoader(f\"{data_path}/MachineLearning-Lecture01.pdf\"),\n",
    "    PyPDFLoader(f\"{data_path}/MachineLearning-Lecture01.pdf\"),\n",
    "    PyPDFLoader(f\"{data_path}/MachineLearning-Lecture03.pdf\")\n",
    "]\n",
    "docs = []\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load())"
   ],
   "id": "1bfd8e7e18c145ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# SPLIT DOCS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1500,\n",
    "    chunk_overlap = 150\n",
    ")\n",
    "splits = text_splitter.split_documents(docs)"
   ],
   "id": "e2e7b06813bc6b29",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Vector DB - Indexing / Store\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# Optional persist_directory to save the database\n",
    "persist_directory = './db/chroma-ML-docs/'\n",
    "\n",
    "vectordb = Chroma.from_documents(\n",
    "    collection_name=\"ml_docs\",\n",
    "    documents=splits,\n",
    "    embedding=embedding_model,\n",
    "    #persist_directory=persist_directory # Optionally persist the database\n",
    ")\n",
    "retriever = vectordb.as_retriever()"
   ],
   "id": "522653aabd8bc7a4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(vectordb._collection.count())",
   "id": "58985b34dd50ba7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Create a RAG chain",
   "id": "fdafbc5f8a2ae265"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Prompt",
   "id": "9586e5c7b990ae0e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Build prompt\n",
    "q_and_a_system_template = \"\"\"Use the following pieces of context to answer the question. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. \n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\"\"\"\n",
    "q_and_a_prompt = ChatPromptTemplate([\n",
    "    (\"system\", q_and_a_system_template),\n",
    "    (\"human\", \"{input}\"),\n",
    "])\n"
   ],
   "id": "648cec5d45827242",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Start with a simple RAG chain",
   "id": "e318f24122264f82"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "combine_docs_chain = create_stuff_documents_chain(llm, q_and_a_prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, combine_docs_chain)"
   ],
   "id": "c8eb4278965d02e4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "question = \"Is probability a class topic?\"\n",
    "result = rag_chain.invoke({\"input\": question})"
   ],
   "id": "74eedc34d62f9252",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "result[\"answer\"]",
   "id": "826006c239d636de",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Get first source document\n",
    "result[\"context\"][0]"
   ],
   "id": "259e86f7e0b859a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Add history aware retrieval \n",
    "\n",
    "The chain we have built uses the input query directly to retrieve relevant context. But in a conversational setting, the user query might require conversational context to be understood. For example, consider this exchange:\n",
    "\n",
    "> Human: \"Is probability a class topic?\"\n",
    ">\n",
    "> AI: \"Yes, probability is a class topic, as the course assumes familiarity with basic probability and statistics.\"\n",
    ">\n",
    "> Human: \"Why are those prerequisites needed?\"\n",
    "\n",
    "In order to answer the second question, our system needs to understand that \"those\" refers to \"probability and statistics.\"\n",
    "\n",
    "We'll need to update two things about our existing app:\n",
    "\n",
    "1. **Prompt**: Update our prompt to support historical messages as an input.\n",
    "2. **Contextualizing questions**: Add a sub-chain that takes the latest user question and reformulates it in the context of the chat history. This can be thought of simply as building a new \"history aware\" retriever. Whereas before we had:\n",
    "query -> retriever\n",
    "Now we will have:\n",
    "(query, conversation history) -> LLM -> rephrased query -> retriever"
   ],
   "id": "35e984a760a8e8a4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")"
   ],
   "id": "d26f5d11de02aae9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Your task: Put it together! \n",
    "Build the final chain using history_aware_retriever. Hint: look at how the basic RAG chain was put together. And look here for further reference: \n",
    "https://python.langchain.com/docs/tutorials/qa_chat_history/#adding-chat-history "
   ],
   "id": "42d6e5b50167e92"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "# TODO: Your task - redefine q_and_a_prompt to take history into account\n",
    "q_and_a_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        # TODO: ...\n",
    "    ]\n",
    ")\n",
    "\n",
    "# TODO: Your task - create the final RAG chain \n",
    "# rag_chain = "
   ],
   "id": "9bea0ce325637826",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Setup the simples possible store for message history \n",
    "**_We'll improve on this in the next (agent) section._**"
   ],
   "id": "4c2ea7b50be29a39"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from typing import List\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "chat_history: List[BaseMessage] = []\n",
    "def add_to_history(human_message: str, ai_message: str):\n",
    "    chat_history.extend(\n",
    "        [\n",
    "            HumanMessage(content=human_message),\n",
    "            AIMessage(content=ai_message),\n",
    "        ]\n",
    "    )"
   ],
   "id": "dcdaaf8a93b63bca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Run it!",
   "id": "dfc8f932505aa7e1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "ai_msg_1 = rag_chain.invoke({\"input\": question, \"chat_history\": chat_history})\n",
    "add_to_history(question, ai_message=ai_msg_1[\"answer\"])\n",
    "\n",
    "second_question = \"Why are those prerequisites needed?\"\n",
    "ai_msg_2 = rag_chain.invoke({\"input\": second_question, \"chat_history\": chat_history})\n",
    "\n",
    "print(ai_msg_2[\"answer\"])"
   ],
   "id": "bea6709acbbf8fe5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Have a look at the trace in LangSmith\n",
    "Example: https://smith.langchain.com/public/7cfa0ffd-90ae-4aa3-8a5a-be7479010a17/r"
   ],
   "id": "bfe606c6ff5fb541"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
