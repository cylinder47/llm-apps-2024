{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 2.5 RAG Question & Answering\n",
    "\n",
    "![RAG - query pipeline](https://python.langchain.com/assets/images/rag_retrieval_generation-1046a4668d6bb08786ef73c56d4f228a.png)"
   ],
   "id": "943ea36a1082641e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Setup\n",
    "\n",
    "### Install dependencies"
   ],
   "id": "c9dd64db952da1d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%pip install python-dotenv~=1.0 docarray~=0.40.0 pypdf~=5.1 --upgrade --quiet\n",
    "%pip install chromadb~=0.5.18 sentence-transformers~=3.3 --upgrade --quiet \n",
    "%pip install langchain~=0.3.7 langchain_openai~=0.2.6 langchain_community~=0.3.5 --upgrade --quiet\n",
    "\n",
    "# If running locally, you can do this instead:\n",
    "#%pip install -r ../requirements.txt"
   ],
   "id": "5e800faf4e660819",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load environment variables",
   "id": "23db6d4042f86418"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# If running in Google Colab, you can use this code instead:\n",
    "# from google.colab import userdata\n",
    "# os.environ[\"AZURE_OPENAI_API_KEY\"] = userdata.get(\"AZURE_OPENAI_API_KEY\")\n",
    "# os.environ[\"AZURE_OPENAI_ENDPOINT\"] = userdata.get(\"AZURE_OPENAI_ENDPOINT\")"
   ],
   "id": "e9f00cf70a4e8556",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setup Chat Model",
   "id": "9c3d11e1151a7e80"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\n",
    "api_version = \"2024-10-01-preview\"\n",
    "llm = AzureChatOpenAI(deployment_name=\"gpt-4o\", temperature=0.0, api_version=api_version)\n",
    "embedding_model = AzureOpenAIEmbeddings(model=\"text-embedding-3-large\", api_version=api_version)"
   ],
   "id": "ba095904eb63506",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setup LangSmith tracing for this notebook",
   "id": "3e0dcb326c5b015b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "# API key etc is in the .env file\n",
    "# my_name = \"Totoro\"\n",
    "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "# os.environ[\"LANGCHAIN_PROJECT\"] = f\"tokyo24-test-{my_name}\""
   ],
   "id": "f3b906942dc243b7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setup path to data ",
   "id": "eca020d02688c907"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "data_path = \"../data\"",
   "id": "e2a050a3283e5523",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Initialize VectorDB",
   "id": "6bde3aec39acd7df"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We've discussed `Document Loading` and `Splitting` as well as `Indexing` and `Retrieval` already.\n",
    "\n",
    "Let's load our vectorDB and set it up as in chapter 2.3. _If you already have a persisted vectorDB, you can skip to \"Vector DB\" below._"
   ],
   "id": "f3bd1965176d16fc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load docs",
   "id": "b50b486cfefcaa04"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# Load PDFs\n",
    "loaders = [\n",
    "    PyPDFLoader(f\"{data_path}/MachineLearning-Lecture01.pdf\"),\n",
    "    PyPDFLoader(f\"{data_path}/MachineLearning-Lecture01.pdf\"),\n",
    "    PyPDFLoader(f\"{data_path}/MachineLearning-Lecture03.pdf\")\n",
    "]\n",
    "docs = []\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load())"
   ],
   "id": "1bfd8e7e18c145ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Split docs",
   "id": "468772c910d8752f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1500,\n",
    "    chunk_overlap = 150\n",
    ")\n",
    "splits = text_splitter.split_documents(docs)"
   ],
   "id": "e2e7b06813bc6b29",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Vector DB - Indexing / Store\n",
   "id": "b9ac9e1802663883"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# Optional persist_directory to save the database\n",
    "persist_directory = './db/chroma-ML-docs/'\n",
    "\n",
    "vectordb = Chroma.from_documents(\n",
    "    collection_name=\"ml_docs\",\n",
    "    documents=splits,\n",
    "    embedding=embedding_model,\n",
    "    #persist_directory=persist_directory # Optionally persist the database\n",
    ")"
   ],
   "id": "522653aabd8bc7a4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(vectordb._collection.count())",
   "id": "58985b34dd50ba7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "question = \"What are major topics for this class?\"\n",
    "docs = vectordb.similarity_search(question,k=3)\n",
    "len(docs)"
   ],
   "id": "2149ee7c35c12b0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Create a RAG chain",
   "id": "fdafbc5f8a2ae265"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Simple RAG chain",
   "id": "3f793332685cd31a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def format_docs(docs: List[Document]) -> str:\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Setup chain using LCEL\n",
    "qa_chain = (\n",
    "        vectordb.as_retriever()\n",
    "        | format_docs \n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    ")"
   ],
   "id": "d5e055290e6210df",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "qa_chain.invoke(question)",
   "id": "76e971446695d82f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Using a prompt",
   "id": "9586e5c7b990ae0e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Build prompt\n",
    "system_template = \"\"\"Use the following pieces of context to answer the question. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. Always say \"thanks for asking!\" at the end of the answer. \n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\"\"\"\n",
    "q_and_a_prompt = ChatPromptTemplate([\n",
    "    (\"system\", system_template),\n",
    "    (\"human\", \"{input}\"),\n",
    "])\n"
   ],
   "id": "648cec5d45827242",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Build a chain with the prompt, injecting the context and question",
   "id": "6c95908ceca83750"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Setup chain using LCEL\n",
    "qa_chain = (\n",
    "    { # This is a shorthand for a RunnableMap / RunnableParallel\n",
    "        \"context\": vectordb.as_retriever() | format_docs,\n",
    "        \"input\": RunnablePassthrough(),\n",
    "    }\n",
    "    | q_and_a_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ],
   "id": "ceb03ec6f26c643f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "question = \"Is probability a class topic?\"",
   "id": "b21226307cc72b82",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "qa_chain.invoke(question)",
   "id": "58245b263e4b0f1f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Alternative - using helper functions to create the chain",
   "id": "e51fde161da26fb7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Create the alternative chain",
   "id": "e318f24122264f82"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "combine_docs_chain = create_stuff_documents_chain(llm, q_and_a_prompt)\n",
    "alt_rag_chain = create_retrieval_chain(vectordb.as_retriever(), combine_docs_chain)"
   ],
   "id": "c8eb4278965d02e4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "alt_result = alt_rag_chain.invoke({\"input\": question})",
   "id": "74eedc34d62f9252",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "alt_result[\"answer\"]",
   "id": "826006c239d636de",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Get first source document\n",
    "alt_result[\"context\"][0]"
   ],
   "id": "259e86f7e0b859a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Have a look at the trace in LangSmith\n",
    "Exammple: https://smith.langchain.com/public/6d3ebe1f-fc1e-434d-90b5-f60e2fe1d286/r"
   ],
   "id": "bfe606c6ff5fb541"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Next step will add chat memory!",
   "id": "9793aef66515ab64"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
